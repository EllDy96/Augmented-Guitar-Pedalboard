{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_21944\\3230636011.py:29: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import scipy as sp \n",
    "from scipy import signal\n",
    "#from signal import butter\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "#import librosa\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input, Reshape , Dropout\n",
    "from keras.regularizers import l1\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#importo funzioni di pre-processing\n",
    "from PreProcessing.processing import *\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('GPU device not found')\n",
    "  #raise SystemError('GPU device not found')    \n",
    "else:\n",
    "  print('I Found a GPU! at: {}'.format(device_name))\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17959731461096736053\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RowPower signal preprocessing\n",
    "here we apply the same preprocessing that the system does in real time before fedding the data into the model. This allow us to train the model with back up datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lenght and shapes of my training datasets = 7, [(30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8)]\n",
      "[[[0.57640807 0.57159955 0.64360317 ... 0.3120852  0.45602244 0.43430842]]\n",
      "\n",
      " [[0.55510371 0.55204205 0.65485674 ... 0.3187992  0.45584321 0.44931653]]\n",
      "\n",
      " [[0.56572256 0.5760748  0.66004097 ... 0.34812471 0.47683057 0.42754662]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.54029447 0.56749725 0.72417368 ... 0.33624016 0.63961251 0.45171062]]\n",
      "\n",
      " [[0.58314127 0.59708936 0.66282275 ... 0.36139836 0.6746243  0.41489827]]\n",
      "\n",
      " [[0.58152226 0.57347291 0.55612877 ... 0.36078098 0.71554158 0.4345401 ]]]\n",
      "The Shape of the Target set:  [(30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5)]\n"
     ]
    }
   ],
   "source": [
    "#RAW path\n",
    "\n",
    "raw_data = {\n",
    "    'arpeggio_raw' : pd.read_csv(\"C:\\\\Users\\david\\OneDrive - Politecnico di Milano\\Documenti\\GItDesktop\\MAE_Thesis\\\\firstPrototype\\secondAcquisition\\\\1 Arpeggio\\Arpeggio.csv\",sep=';'),\n",
    "    'strumming_raw': pd.read_csv(\"C:\\\\Users\\david\\OneDrive - Politecnico di Milano\\Documenti\\GItDesktop\\MAE_Thesis\\\\firstPrototype\\secondAcquisition\\\\2 Strumming\\Strumming.csv\",sep=';'),\n",
    "    'bending_raw' :  pd.read_csv(\"C:\\\\Users\\david\\OneDrive - Politecnico di Milano\\Documenti\\GItDesktop\\MAE_Thesis\\\\firstPrototype\\secondAcquisition\\\\3 Bending\\Bending.csv\",sep=';'),\n",
    "    'pullOffHammerOn_raw': pd.read_csv(\"C:\\\\Users\\david\\OneDrive - Politecnico di Milano\\Documenti\\GItDesktop\\MAE_Thesis\\\\firstPrototype\\secondAcquisition\\\\4 PullOffHammerOn\\PullOffHammerOn.csv\",sep=';'),\n",
    "    'tapping_raw': pd.read_csv(\"C:\\\\Users\\david\\OneDrive - Politecnico di Milano\\Documenti\\GItDesktop\\MAE_Thesis\\\\firstPrototype\\secondAcquisition\\\\5 Tapping\\\\tapping.csv\",sep=';'),\n",
    "    'strongPick_raw':  pd.read_csv(\"C:\\\\Users\\david\\OneDrive - Politecnico di Milano\\Documenti\\GItDesktop\\MAE_Thesis\\\\firstPrototype\\secondAcquisition\\\\6 StrongPick\\\\strongPick.csv\",sep=';'),\n",
    "    'doublePick_raw' : pd.read_csv(\"C:\\\\Users\\david\\OneDrive - Politecnico di Milano\\Documenti\\GItDesktop\\MAE_Thesis\\\\firstPrototype\\secondAcquisition\\\\7 DoublePick\\\\doublePick.csv\",sep=';') ,\n",
    "}\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for i in raw_data:\n",
    "  raw_data[i]= raw_data[i].iloc[5000:35000,3:].values # excluding the first and the last 5 second of samples and trasmorming it it np.array \n",
    "  raw_data[i] = scaler.fit_transform(raw_data[i])#scales the data from (0,1)\n",
    "  raw_data[i]= np.reshape(raw_data[i],(raw_data[i].shape[0], 1, raw_data[i].shape[1]))\n",
    "print('The lenght and shapes of my training datasets = {}, {}'.format(len(raw_data), [raw_data[key].shape for key in raw_data]))\n",
    "print(raw_data['arpeggio_raw'])\n",
    "\n",
    "#remanber to set the same keys name of the trainig dictonary and the target dictionary\n",
    "#doig so you can apply the same dimensionality[0] of the training set to the first dimension of the testing set \n",
    "\n",
    "#Target Set\n",
    "target_dict = {\n",
    "    'arpeggio_raw': np.array([0.5,0.3, 0.2, 0.3, 0.5]),\n",
    "    'strumming_raw': np.array([0.6,0.2, 0.8, 0.7, 0.3]),\n",
    "    'bending_raw': np.array([0.2,0.1, 0.1, 0.5, 0.2]),\n",
    "    'strongPick_raw': np.array([0.4,0.5, 0.3, 0.4, 0.1]),\n",
    "    'tapping_raw': np.array([0.3,0.2, 0.6, 0.6, 0.7]),\n",
    "    'pullOffHammerOn_raw': np.array([0.7,0.7, 0.8, 0.2, 0.4]),\n",
    "    'doublePick_raw': np.array([0.8,0.4, 0.9, 0.1, 0.8]),\n",
    "}\n",
    "#populate the dictionaty with the right shapes before using them as target set\n",
    "# i have to create a target set with 8 rows and 5 columns \n",
    "\n",
    "for i in target_dict:  \n",
    "  target_dict[i]= np.tile(target_dict[i], (raw_data[i].shape[0],1))\n",
    "\n",
    "print(\"The Shape of the Target set: \",[i.shape for i in target_dict.values()])\n",
    "\n",
    "#arpeggio_row= pd.read_csv(arpeggio_path_raw,sep= ';')\n",
    "#arpeggio_row = arpeggio_row.iloc[:,3:]\n",
    "\n",
    "#first I hae to remouve the first 3 column that are not data but timestemp ID and Sequence\n",
    "#print(arpeggio_row)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i have imported and shaeped the dataset i need to preprocess them wiht a butter band pass filter. It's good to plot your training set if you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(12,8))\n",
    "#plt.plot(raw_data['arpeggio_raw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos = signal.butter(5, [30,300], 'bandpass', fs=1000, output='sos') #second order section filter band pass\n",
    "\n",
    "#applying band pass filter \n",
    "for i in raw_data:\n",
    "    raw_data[i]= signal.sosfilt(sos, raw_data[i].astype(np.float32))\n",
    "\n",
    "print('Training dataset lenght {} and shape {}'.format(len(raw_data), [raw_data[key].shape for key in raw_data]))\n",
    "\n",
    "print(raw_data['arpeggio_raw'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the RMS \n",
    "Now we apply a window rms to obtain the same number of sample but in a RMS form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             g00  g01  g02  g03  g10  g11  g12  g13\n",
      "0   -5304.710925  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "1     425.610817  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "2     101.243193  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "3    8925.926500  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "4    6448.940477  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "..           ...  ...  ...  ...  ...  ...  ...  ...\n",
      "495 -6220.534876  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "496 -1964.807550  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "497  8493.520523  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "498  2494.440886  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "499  3339.880792  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "\n",
      "[500 rows x 8 columns]\n",
      "             g00  g01  g02  g03  g10  g11  g12  g13\n",
      "0   -4914.564746  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "1   -8827.948709  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "2     -47.334185  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "3   -3088.538524  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "4    8838.718929  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "..           ...  ...  ...  ...  ...  ...  ...  ...\n",
      "495  5321.819378  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "496 -2575.999390  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "497 -6543.900007  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "498 -8597.177117  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "499  8628.133080  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
      "\n",
      "[500 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#prove di funzionamento per popolare il \n",
    "df_toLSTM= pd.DataFrame(columns=['g00', 'g01', 'g02', 'g03','g10', 'g11', 'g12', 'g13'])\n",
    "g00= np.random.uniform(low=-10000, high=10000, size=500)\n",
    "df_toLSTM['g00'] = g00\n",
    "\n",
    "print(df_toLSTM)\n",
    "\n",
    "df_toLSTM['g00'] = np.random.uniform(low=-10000, high=10000, size=500)\n",
    "print(df_toLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The new dataset size  \n",
      " (500, 8)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RNN_input_preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m The new dataset size \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, df_toLSTM_1\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     28\u001b[0m \u001b[39m#print('\\n The new normalized dataset  ', '\\n', df_toLSTM_1)\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m df_arpeggio_RMS \u001b[39m=\u001b[39m RNN_input_preprocessing(df_toLSTM_1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RNN_input_preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "random_data = {\n",
    "    'g00': np.random.uniform(low=-10000, high=10000, size=500),\n",
    "    'g01': np.random.uniform(low=-10000, high=10000, size=500),\n",
    "    'g02': np.random.uniform(low=-10000, high=10000, size=500),\n",
    "    'g03': np.random.uniform(low=-10000, high=10000, size=500),\n",
    "    'g10': np.random.uniform(low=-10000, high=10000, size=500),\n",
    "    'g11': np.random.uniform(low=-10000, high=10000, size=500),\n",
    "    'g12': np.random.uniform(low=-10000, high=10000, size=500),\n",
    "    'g13': np.random.uniform(low=-10000, high=10000, size=500)\n",
    "}\n",
    "\n",
    "df_toLSTM_1= pd.DataFrame(random_data)\n",
    "'''\n",
    "df_toLSTM_1= df_toLSTM_1.values\n",
    "#if the nuber of initial sample if divisible by the group size you can create groups of group_size only using reshape(-1, group_size, 8)\n",
    "df_toLSTM_1= df_toLSTM_1.reshape(-1,25,8)\n",
    "print(df_toLSTM_1.shape)ù\n",
    "'''\n",
    "# Reshape the DataFrame into the desired shape\n",
    "#reshaped_df = df_toLSTM_1.values.reshape(-1, 32, 8)\n",
    "#df_toLSTM_1= scaler.fit_transform(df_toLSTM_1)\n",
    "#scaler= MinMaxScaler()\n",
    "#for i in random_data:\n",
    "#    random_data[i]= scaler.fit_transform(random_data[i])\n",
    "#df_toLSTM_1 = df_toLSTM_1.values\n",
    "\n",
    "print('\\n The new dataset size ', '\\n', df_toLSTM_1.shape)\n",
    "#print('\\n The new normalized dataset  ', '\\n', df_toLSTM_1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok ho definito un dataset di prova. ora devo solo provare a darlo in pasto al modello per vedere se me lo predice bene.\n",
    "\n",
    "Whit the input_preprocessing function I pack my data before feeding them to the model: I adacts my inputs dataframe to have the model imput shape, I covert the row data into a rms and I filter them wiht a notch and a band pass filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_RMS with initial shape  (376, 8)\n",
      "The final shape: (11, 32, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def RNN_input_preprocessing(raw_dataframe, fs= 1000): \n",
    "    '''\n",
    "    creates a dataframe containing filtered Emg signals (Df_Emg_Filtered), a dataframe containing smoothed signals (Df_Savitzky) a dataframe containing RMS signals (Df_RMS) and a datafreme containing MNF signals (Df_MNF)\n",
    "    Args:\n",
    "        only_DF_RMS(bool): if True only Df_RMS will be compute otherwise Df_Emg_Filtered, Df_RMS, Df_Savitzky and Df_RMS will be computed \n",
    "    '''\n",
    "    #Defining the band pass parameters and the windows size\n",
    "    lowcut = 30.0 #inizializzazione frequenza taglia basso\n",
    "    highcut = 300.0  #inizializzazione frequenza taglia alto (verrà comunque detrminata automaticamente all'apertura del file EMG)\n",
    "    order=5 #ordine del filtro di butterworth\n",
    "    rms_window_size=125 #numero di campioni su cui effettuare la convoluzione per il calcolo del RMS\n",
    "\n",
    "    #CALCOLO DF SEGNALI FILTRATI\n",
    "    #casting to int before applying the bandpas, othetwise it rainses ad exeption\n",
    "    #for i in range(len(raw_dataframe.columns[:-1])):\n",
    "    #   raw_dataframe[raw_dataframe.columns[i]] = raw_dataframe[raw_dataframe.columns[i]].astype('int64')\n",
    "\n",
    "    \n",
    "    #applicazione filtro passa panda e di notch su ogni colonna (canale) del dataframe Raw\n",
    "    #raw_dataframe= raw_dataframe.fillna(0).astype(int)\n",
    "    #now we apply the notch filter feeding it with the prefiltered bandapassed stream \n",
    "    df_Emg_Filtered= raw_dataframe.apply(lambda x: Implement_Notch_Filter(fs,1,50, None, 3, 'butter', \n",
    "                                        butter_bandpass_filter(x,lowcut,highcut, fs, order=order)))\n",
    "\n",
    "    \n",
    "    #CALCOLO DF SEGNALI RMS\n",
    "    def df_rms(vect):\n",
    "        rect=abs(vect.fillna(0).values)\n",
    "        for i in range(25):\n",
    "            rect[i]=int(1)\n",
    "        return window_rms(rect,rms_window_size)\n",
    "\n",
    "    #applicazione della funzione df_rms su ogni colonna (canale) del dataframe contente i segnali filtrati\n",
    "    #apply the RMS to the band passed dataframe\n",
    "    df_RMS= df_Emg_Filtered.apply(lambda x: df_rms(x))\n",
    "    df_RMS= df_RMS.round(2)                                                                                                                                                                                                                                                                                                                                                     \n",
    "\n",
    "    \n",
    "    #tempo del segnale rms è uguale a quello emg solo tagliato all'inizio e alla fine per via della convoluzione 'valid'\n",
    "    #il primo campione del RMS è stato assegnato a metà della prima finestra\n",
    "    #tempo_rms= raw_dataframe['TIMESTAMP'][int(rms_window_size/2):-int(rms_window_size/2)+1] \n",
    "    \n",
    "    print('df_RMS with initial shape ', df_RMS.shape) # '\\n', df_RMS, '\\n', type(df_RMS))\n",
    "\n",
    "    #scaling the dataset \n",
    "    #raw_dataframe= scaler.fit_transform(raw_dataframe) # it return an np array \n",
    "    #df_RMS= df_RMS.values\n",
    "\n",
    "    #print('the shape of the df array: ', df_RMS.shape)\n",
    "    #scaling the value from (0,1)ù\n",
    "    scaler = MinMaxScaler()\n",
    "    df_RMS= scaler.fit_transform(df_RMS)\n",
    "\n",
    "    #df_RMS[i]= df_RMS[i].values\n",
    "\n",
    "    sequence_length = 32\n",
    "    # Print the shape of the raw data before reshaping\n",
    "    #print(\"Raw Data Shape:\", df_RMS.shape[0])\n",
    "\n",
    "    # Calculate the number of groups\n",
    "    num_groups = df_RMS.shape[0] // sequence_length\n",
    "    # Print the number of groups\n",
    "    #print(\"Number of Groups:\", num_groups)\n",
    "    \n",
    "    casting_index= num_groups * sequence_length\n",
    "    # Calculate the number of remaining elements\n",
    "    #num_remaining = df_RMS[i].shape[0] % sequence_length\n",
    "    \n",
    "    # Reshape the input data into a 3D matrix\n",
    "    input_sequences = np.reshape(df_RMS[:casting_index], (num_groups, sequence_length, df_RMS.shape[1]))\n",
    "    df_RMS = input_sequences     \n",
    "\n",
    "\n",
    "    # The shape of 'input_sequences' will be (num_groups + 1, sequence_length, num_features)\n",
    "    print('The final shape:' , df_RMS.shape)\n",
    "\n",
    "    return df_RMS\n",
    "df_arpeggio_RMS = RNN_input_preprocessing(df_toLSTM_1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Training and Target set\n",
    " \n",
    "To define the target set, it is necessary to ensure that its first dimension matches that of the training set (the number of rows). We have a training set of shape(num_samples,1, 8), it means with 8 features each wiht dimension 1. We want to have a 5 floats outputs so we need to define a target set of num_sample rows and 5 targets ( 5 output features). \n",
    "\n",
    "The idea is create two dictionary: the training and the target. I need them wiht the same keys name to guarantee that when I load a new acquisition of one of the training dict element, i will concatenate the last and the dimension of the corresponding target dict will increase the number of samples accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lenght and shapes of my training datasets = 7, [(30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8)]\n",
      "mean and variance of arpeggio  = 2236.176, 1305701.184\n",
      "mean and variance of strumming  = 4943.854, 26904060.131\n",
      "mean and variance of bending  = 3656.116, 151639304.146\n",
      "mean and variance of double pick  = 2294.655, 2121090.473\n",
      "mean and variance of strong pick  = 2088.049, 2635636.172\n",
      "mean and variance of tapping  = 2379.778, 2249018.599\n",
      "mean and variance of pullOff Hammer on  = 2526.514, 6551681.842\n",
      "The Shape of the Target set:  [(30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5)]\n"
     ]
    }
   ],
   "source": [
    "#completare con valori sensati\n",
    "\n",
    "#Training set \n",
    "pd_dataframe= {\n",
    "    'arpeggio': pd.read_csv('csv_pd_dataframe\\df_arpeggio_rms'),\n",
    "    'strumming': pd.read_csv('csv_pd_dataframe/df_strumming_rms'),\n",
    "    'bending': pd.read_csv('csv_pd_dataframe/df_bending_rms'),\n",
    "    'double pick':pd.read_csv('csv_pd_dataframe/df_doublePick_rms'),\n",
    "    'strong pick':pd.read_csv('csv_pd_dataframe/df_strongPick_rms'),\n",
    "    'tapping':pd.read_csv('csv_pd_dataframe/df_tapping_rms'),\n",
    "    'pullOff Hammer on': pd.read_csv('csv_pd_dataframe/df_pullOffHammerOn_rms')\n",
    "}\n",
    "#reshaping all the pd dataset to be (samples,time steps, features) that in our case is (30000, 1, 8) shape \n",
    "for i in pd_dataframe:\n",
    "  pd_dataframe[i]= pd_dataframe[i].iloc[5000:35000].values # excluding the first and the last 5 second of samples and trasmorming it it np.array \n",
    "  pd_dataframe[i]= np.reshape(pd_dataframe[i],(pd_dataframe[i].shape[0], 1, pd_dataframe[i].shape[1]))\n",
    "print('The lenght and shapes of my training datasets = {}, {}'.format(len(pd_dataframe), [pd_dataframe[key].shape for key in pd_dataframe]))\n",
    "\n",
    "#Printing some statistics \n",
    "for i in pd_dataframe.keys():\n",
    "  print('mean and variance of', i ,' = {:.3f}, {:.3f}'.format(np.mean(pd_dataframe[i]), np.var(pd_dataframe[i])))\n",
    "\n",
    "#Target Set\n",
    "target_dict = {\n",
    "    'arpeggio': np.array([0.5,0.3, 0.2, 0.3, 0.5]),\n",
    "    'strumming': np.array([0.6,0.2, 0.8, 0.7, 0.3]),\n",
    "    'bending': np.array([0.2,0.1, 0.1, 0.5, 0.2]),\n",
    "    'strongPick': np.array([0.4,0.5, 0.3, 0.4, 0.1]),\n",
    "    'tapping': np.array([0.3,0.2, 0.6, 0.6, 0.7]),\n",
    "    'pullOffHammerOn': np.array([0.7,0.7, 0.8, 0.2, 0.4]),\n",
    "    'doublePick': np.array([0.8,0.4, 0.9, 0.1, 0.8]),\n",
    "}\n",
    "#populate the dictionaty with the same sample as the training set before using them as target set, \n",
    "# i have to create a target set with 30000 rows and 5 columns (samples, outputs)\n",
    "for i in target_dict:  \n",
    "  target_dict[i]= np.tile(target_dict[i], (30000,1))\n",
    "\n",
    "print(\"The Shape of the Target set: \",[i.shape for i in target_dict.values()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset packing\n",
    "Here I pack my input data to feed them to the Model during the training process. Furthemore we split the Training dataset in two part, one for the training process and one for the evalution, called Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The traing set shape (168000, 1, 8) , the test set shape (42000, 1, 8), the tartet set shape (168000, 5) , the target testing set shape (42000, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "concatenated_training_datasets= np.concatenate(([pd_dataframe[key] for key in pd_dataframe.keys()]),axis=0)\n",
    "#print(x_train.shape)\n",
    "concatenated_target_datasets= np.concatenate(([target_dict[key] for key in target_dict.keys()]), axis= 0)\n",
    "#print(y_train.shape)\n",
    "#splitting the dataset into a training and testing dataset\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(concatenated_training_datasets, concatenated_target_datasets, test_size=0.2, random_state=42)\n",
    "print('The traing set shape {} , the test set shape {}, the tartet set shape {} , the target testing set shape {}'.format(X_train.shape ,X_test.shape, Y_train.shape, Y_test.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Sequential Network\n",
    "Un idea per migliorare le perfomance sarebbe provare a normalizzare l'input da 0,1 0 da -1 a 1 e provare ad allenare la rete con i dati row invece che con l'RMS. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output shape:  (None, 5)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 1, 64)             18688     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,037\n",
      "Trainable params: 52,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#By setting the input shape to (8, 1), you are specifying that each sample consists of 8 time steps, each time step consisting of a single feature\n",
    "# Add an input dense layer with 8 neurons, one for each column in the input dataset\n",
    "\n",
    "# Add an LSTM layer with 64 units\n",
    "model.add(LSTM(64, input_shape= (1,8),return_sequences=True, kernel_regularizer=l1(0.1))) \n",
    "# when you want to stack multiple LSTM \n",
    "#you have to set the return_sequences=True to not have shape problems. \n",
    "#return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False\n",
    "\n",
    "model.add(Dropout(0.3))  # add a dropout layer with dropout rate of 0.2\n",
    "model.add(LSTM(64))\n",
    "# Add a dense output layer with 5 units and linear activation\n",
    "model.add(Dense(5, activation='linear'))  \n",
    "\n",
    "print('The output shape: ',model.output_shape)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "To compile the network (before training it) we have to choose the Optimizer and the loss function: \n",
    "\n",
    "The optimizer is the algorithm used to update the weights of the model during training, and 'adam' is a popular optimizer that adapts the learning rate of each weight based on the first and second moments of the gradients.\n",
    "\n",
    "The loss function is used to measure how well the model is performing on the training data and guides the optimization process. 'mse' stands for mean squared error, which is a common loss function used for regression problems where the goal is to minimize the difference between the predicted and actual values.\n",
    "\n",
    "Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "\n",
    "Learning rate: The learning rate determines how much the model weights are updated during training. A larger learning rate can lead to faster convergence, but can also cause the model to overshoot the minimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "\n",
    "myOptimizer = keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer= myOptimizer, loss='mse', metrics=['mse'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,Y_train, validation_split=0.25, batch_size= 64, epochs=75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The validation_split argument in Keras fit method specifies the fraction of the training data to be used as validation data. The model will not be trained on the validation data, but rather the validation data will be used to evaluate the model's training performance over the epochs on data it has not seen before.\n",
    "\n",
    "For example, if validation_split=0.2, then 20% of the training data will be used as validation data and the remaining 80% will be used as the training data. The validation data will be used to evaluate the model's performance after each epoch during training. The validation set is a portion of the training set to evaluate the training process, instead to evaluate the network we have to use a different dataset called testing dataset.\n",
    "\n",
    "Batch size: The batch size determines the number of training examples that are used in each training iteration. Larger batch size can lead to faster training, but can also lead to slower convergence and poorer generalization.\n",
    "\n",
    "The number of LSTM layers: Increasing the number of layers can help capture more complex relationships in the data, but can also increase the risk of overfitting.\n",
    "\n",
    "The number of LSTM units: Increasing the number of neurons can help the model capture more complex patterns in the data, but can also increase the risk of overfitting.\n",
    "\n",
    "Learning rate: The learning rate determines how much the model weights are updated during training. A larger learning rate can lead to faster convergence, but can also cause the model to overshoot the minimum.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the perormance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can track the training accurancy to avoid overfitting by plotting the loss function and the MSE for each epoch. The MSE is a common metric to evaluate regession task. The lower the loss, the better the model performance. The purpose of the loss function is to evaluate how well the model is learning the patterns in the data and adjusting its weights and biases to make accurate predictions. So, when you train a model, you try to minimize the loss function by adjusting the model's parameters. In other evaluation metrics, such as accuracy or F1 score, the higher the value, the better the model performance.\n",
    "Ploting the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.title('arpeggio')\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['val_mse'])\n",
    "plt.title('Model MSE')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "Testing the model whit a testing set computing the loss function for an unseen dataset. \n",
    "The testing dataset have to be useen data but whith a trained target set, you can use a dataset belonging to a target that you did not train. In the training the network see the label, in the tasting set it see the label only at the end, but shold be data from a studied label otherwise it has no sense. \n",
    "it has to be data belonging to one of the category which you use during the training but unseen during the training stage.\n",
    "you have to split the training dataset to obtai a testing dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## Model evaluation \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39mevaluate(X_test,Y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "## Model evaluation \n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of new data\n",
    "Now we have to actualy try the regression, we could use the method model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(376, 1, 8)\n",
      "12/12 [==============================] - 1s 2ms/step\n",
      "[[0.57763714 0.46410373 0.7094731  0.66726536 0.13209713]\n",
      " [0.57763714 0.46410373 0.7094731  0.66726536 0.13209713]\n",
      " [0.57763714 0.46410373 0.7094731  0.66726536 0.13209713]\n",
      " ...\n",
      " [0.5999903  0.2999681  0.79445815 0.6954058  0.30008966]\n",
      " [0.5999903  0.2999681  0.79445815 0.6954058  0.30008966]\n",
      " [0.5999903  0.2999681  0.79445815 0.6954058  0.30008966]] (376, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model('sEMG_regression.h5')\n",
    "\n",
    "# Reshape the data to match the input shape of the model\n",
    "\n",
    "#new_data= pd_dataframe['bending'][2]\n",
    "new_data= df_arpeggio_RMS.values\n",
    "\n",
    "new_data= np.reshape(new_data,(new_data.shape[0],1, new_data.shape[1]))\n",
    "print(new_data.shape) \n",
    "#new_data = new_data.reshape(1, -1)\n",
    "\n",
    "# Make a prediction using the model\n",
    "prediction = model.predict(new_data)\n",
    "\n",
    "print(prediction, prediction.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the model \n",
    "Now we want to save the model to use it in RawPower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('sEMG_regression.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could load again the model from everywhere else, be sure to import the same libraries imported at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model('sEMG_regression.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
