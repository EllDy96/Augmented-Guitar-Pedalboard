{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_5908\\193200553.py:24: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print('GPU device not found')\n",
    "  #raise SystemError('GPU device not found')    \n",
    "else:\n",
    "  print('I Found a GPU! at: {}'.format(device_name))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "#import librosa\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input, Reshape , Dropout\n",
    "from keras.regularizers import l1\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Training and Target set\n",
    " \n",
    "To define the target set, it is necessary to ensure that its first dimension matches that of the training set (the number of rows). We have a training set of shape(num_samples,1, 8), it means with 8 features each wiht dimension 1. We want to have a 5 floats outputs so we need to define a target set of num_sample rows and 5 targets ( 5 output features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape of the Target set:  [(30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5)]\n",
      "The lenght and shapes of my training datasets = 7, [(30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8)]\n",
      "mean and variance of arpeggio  = 2236.176, 1305701.184\n",
      "mean and variance of strumming  = 4943.854, 26904060.131\n",
      "mean and variance of bending  = 3656.116, 151639304.146\n",
      "mean and variance of double pick  = 2294.655, 2121090.473\n",
      "mean and variance of strong pick  = 2088.049, 2635636.172\n",
      "mean and variance of tapping  = 2379.778, 2249018.599\n",
      "mean and variance of pullOff Hammer on  = 2526.514, 6551681.842\n"
     ]
    }
   ],
   "source": [
    "#completare con valori sensati\n",
    "\n",
    "#Training set \n",
    "pd_dataframe= {\n",
    "    'arpeggio': pd.read_csv('csv_pd_dataframe\\df_arpeggio_rms'),\n",
    "    'strumming': pd.read_csv('csv_pd_dataframe/df_strumming_rms'),\n",
    "    'bending': pd.read_csv('csv_pd_dataframe/df_bending_rms'),\n",
    "    'double pick':pd.read_csv('csv_pd_dataframe/df_doublePick_rms'),\n",
    "    'strong pick':pd.read_csv('csv_pd_dataframe/df_strongPick_rms'),\n",
    "    'tapping':pd.read_csv('csv_pd_dataframe/df_tapping_rms'),\n",
    "    'pullOff Hammer on': pd.read_csv('csv_pd_dataframe/df_pullOffHammerOn_rms')\n",
    "}\n",
    "#shaping all the pd dataset to a (30000, 8)\n",
    "for i in pd_dataframe:\n",
    "  pd_dataframe[i]= pd_dataframe[i].iloc[5000:35000].values # excluding the first and the last 5 second of samples\n",
    "  pd_dataframe[i]= np.reshape(pd_dataframe[i],(pd_dataframe[i].shape[0], 1, pd_dataframe[i].shape[1]))\n",
    "print('The lenght and shapes of my training datasets = {}, {}'.format(len(pd_dataframe), [pd_dataframe[key].shape for key in pd_dataframe]))\n",
    "\n",
    "#Printing some statistics \n",
    "for i in pd_dataframe.keys():\n",
    "  print('mean and variance of', i ,' = {:.3f}, {:.3f}'.format(np.mean(pd_dataframe[i]), np.var(pd_dataframe[i])))\n",
    "\n",
    "#Target Set\n",
    "target_dict = {\n",
    "    'arpeggio': np.array([0.5,0.3, 0.2, 0.3, 0.5]),\n",
    "    'strumming': np.array([0.6,0.2, 0.8, 0.7, 0.3]),\n",
    "    'bending': np.array([0.2,0.1, 0.1, 0.5, 0.2]),\n",
    "    'strongPick': np.array([0.4,0.5, 0.3, 0.4, 0.1]),\n",
    "    'tapping': np.array([0.3,0.2, 0.6, 0.6, 0.7]),\n",
    "    'pullOffHammerOn': np.array([0.7,0.7, 0.8, 0.2, 0.4]),\n",
    "    'doublePick': np.array([0.8,0.4, 0.9, 0.1, 0.8]),\n",
    "}\n",
    "#populate the dictionaty with the right shapes before using them as target set\n",
    "# i have to create a target set with 8 rows and 5 columns \n",
    "for i in target_dict:  \n",
    "  target_dict[i]= np.tile(target_dict[i], (30000,1))\n",
    "\n",
    "print(\"The Shape of the Target set: \",[i.shape for i in target_dict.values()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dataset packing\n",
    "Here I pack my input data to feed them to the Model during the training process. Furthemore we split the Training dataset in two part, one for the training process and one for the evalution, called Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168000, 1, 8) (42000, 1, 8) (168000, 5) (42000, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "concatenated_training_datasets= np.concatenate(([pd_dataframe[key] for key in pd_dataframe.keys()]),axis=0)\n",
    "#print(x_train.shape)\n",
    "concatenated_target_datasets= np.concatenate(([target_dict[key] for key in target_dict.keys()]), axis= 0)\n",
    "#print(y_train.shape)\n",
    "#splitting the dataset into a training and testing dataset\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(concatenated_training_datasets, concatenated_target_datasets, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Sequential Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output shape:  (None, 5)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 1, 64)             18688     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,037\n",
      "Trainable params: 52,037\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#By setting the input shape to (8, 1), you are specifying that each sample consists of 8 time steps, each time step consisting of a single feature\n",
    "# Add an input dense layer with 8 neurons, one for each column in the input dataset\n",
    "\n",
    "# Add an LSTM layer with 64 units\n",
    "model.add(LSTM(64, input_shape= (1,8),return_sequences=True, kernel_regularizer=l1(0.1))) # when you want to stack multiple LSTM \n",
    "#you have to set the return_sequences=True to not have shape problems. \n",
    "\n",
    "model.add(Dropout(0.3))  # add a dropout layer with dropout rate of 0.2\n",
    "model.add(LSTM(64))\n",
    "# Add a dense output layer with 5 units and linear activation\n",
    "model.add(Dense(5, activation='linear')) #return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False. IT should be set true only in the output layer \n",
    "\n",
    "print('The output shape: ',model.output_shape)\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "To compile the network (before training it) we have to choose the Optimizer and the loss function: \n",
    "\n",
    "The optimizer is the algorithm used to update the weights of the model during training, and 'adam' is a popular optimizer that adapts the learning rate of each weight based on the first and second moments of the gradients.\n",
    "\n",
    "The loss function is used to measure how well the model is performing on the training data and guides the optimization process. 'mse' stands for mean squared error, which is a common loss function used for regression problems where the goal is to minimize the difference between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,Y_train, validation_split=0.25, epochs=75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The validation_split argument in Keras fit method specifies the fraction of the training data to be used as validation data. The model will not be trained on the validation data, but rather the validation data will be used to evaluate the model's training performance over the epochs on data it has not seen before.\n",
    "\n",
    "For example, if validation_split=0.2, then 20% of the training data will be used as validation data and the remaining 80% will be used as the training data. The validation data will be used to evaluate the model's performance after each epoch during training. The validation set is a portion of the training set to evaluate the training process, instead to evaluate the network we have to use a different dataset called testing dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the perormance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can track the training accurancy to avoid overfitting by plotting the loss function and the MSE for each epoch. The MSE is a common metric to evaluate regession task. The lower the loss, the better the model performance. The purpose of the loss function is to evaluate how well the model is learning the patterns in the data and adjusting its weights and biases to make accurate predictions. So, when you train a model, you try to minimize the loss function by adjusting the model's parameters. In other evaluation metrics, such as accuracy or F1 score, the higher the value, the better the model performance.\n",
    "Ploting the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.title('arpeggio')\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['val_mse'])\n",
    "plt.title('Model MSE')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "Testing the model whit a testing set computing the loss function for an unseen dataset. \n",
    "The testing dataset have to be useen data but whith a trained target set, you can use a dataset belonging to a target that you did not train. In the training the network see the label, in the tasting set it see the label only at the end, but shold be data from a studied label otherwise it has no sense. \n",
    "it has to be data belonging to one of the category which you use during the training but unseen during the training stage.\n",
    "you have to split the training dataset to obtai a testing dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## Model evaluation \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39mevaluate(X_test,Y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "## Model evaluation \n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of new data\n",
    "Now we have to actualy try the regression, we could use the method model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to match the input shape of the model\n",
    "new_data= pd_dataframe['bending'][2]\n",
    "\n",
    "new_data= np.reshape(new_data,(1,new_data.shape[0], new_data.shape[1]))\n",
    "print(new_data.shape)\n",
    "#new_data = new_data.reshape(1, -1)\n",
    "\n",
    "# Make a prediction using the model\n",
    "prediction = model.predict(new_data)\n",
    "print(prediction, prediction.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the model \n",
    "Now we want to save the model to use it in RawPower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('sEMG_regression.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could load again the model from everywhere else, be sure to import the same libraries imported at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = load_model('sEMG_regression.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 8)\n",
      "1/1 [==============================] - 1s 551ms/step\n",
      "[[0.4121541  0.32937828 0.86067164 0.7390499  0.23566648]] (1, 5)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data to match the input shape of the model\n",
    "new_data= pd_dataframe['bending'][2]\n",
    "\n",
    "new_data= np.reshape(new_data,(1,new_data.shape[0], new_data.shape[1]))\n",
    "print(new_data.shape)\n",
    "#new_data = new_data.reshape(1, -1)\n",
    "\n",
    "# Make a prediction using the model\n",
    "prediction = model.predict(new_data)\n",
    "print(prediction, prediction.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
