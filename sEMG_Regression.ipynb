{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EllDy96/MAE_Thesis/blob/main/sEMG_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the libraries and creating a connection wiht my google drive to import the Datasets."
      ],
      "metadata": {
        "id": "xsnPhsWHdQ6z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmqcU7j-mtnC",
        "outputId": "78be3666-6430-4362-c99d-78d5b869f4d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I Found a GPU! at: /device:GPU:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-549338ee2200>:22: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#!pip install tensorflow==2.12.0\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "  #raise SystemError('GPU device not found')    \n",
        "else:\n",
        "  print('I Found a GPU! at: {}'.format(device_name))\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Input, Reshape , Dropout\n",
        "from sklearn import preprocessing\n",
        "from keras.regularizers import l1\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "plt.style.use('seaborn')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # now I can import files from my Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "AvxS_SoDdLQ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eaVabSiRacZ"
      },
      "outputs": [],
      "source": [
        "#!nvidia-smi # it list the availabe GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GDXV9Av3Z4R"
      },
      "source": [
        "# Better to use a LSTM \n",
        "We want to track a time series input data, better handle by LSTM Recurrent NN. Now let's try to implment it \n",
        "First, I need to define your LSTM model architecture using Keras. The architecture can be defined using the Sequential API in Keras. \n",
        "\n",
        "In my case I have an 8  input channel, each linked to a specific sEMG stream coming from a specific muscle.In DL the column of the training set are called features, hence I have 8 feautures. \n",
        "To enable the LSTM network to keep track of the different activation amounts between the eight channels we need to define a input Dense layer whit shape (num_smaple, 8). We feed each of the 8 neurons whit a specific channel. Then we add a LSTM with 64 neurons and a final Dense layer with 5 neurons and a sigmoid activation function to span the result from 0 to one. the sigmoid function maps any real-valued number to a value between 0 and 1, it is a commonly used nonlinear activation function in neural networks. It is defined as:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I2PJDZK_WSi"
      },
      "source": [
        "### Importing the sEMG pandas datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIwM677bIUkE"
      },
      "source": [
        " \n",
        "To define the target set, it is necessary to ensure that its first dimension matches that of the training set (the number of rows). We have a training set of shape(num_samples,1, 8), it means with 8 features each wiht dimension 1. We want to have a 5 floats outputs so we need to define a target set of num_sample rows and 5 targets (5 output features). "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set and the target set has to have the same number of rows, in this case we select 30000 rows that rappresents 30000 time steps of 8 sample each. "
      ],
      "metadata": {
        "id": "nbhIOgkhz8h7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzqZcUiRdwKy",
        "outputId": "4ed4a54b-cc19-42bf-a0cc-4d22811dd97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5), (30000, 5)]\n"
          ]
        }
      ],
      "source": [
        "#completare con valori sensati\n",
        "target_dict = {\n",
        "    'arpeggio': np.array([0.5,0.3, 0.2, 0.3, 0.5]),\n",
        "    'strumming': np.array([0.6,0.2, 0.8, 0.7, 0.3]),\n",
        "    'bending': np.array([0.2,0.1, 0.1, 0.5, 0.2]),\n",
        "    'strongPick': np.array([0.4,0.5, 0.3, 0.4, 0.1]),\n",
        "    'tapping': np.array([0.3,0.2, 0.6, 0.6, 0.7]),\n",
        "    'pullOffHammerOn': np.array([0.7,0.7, 0.8, 0.2, 0.4]),\n",
        "    'doublePick': np.array([0.8,0.4, 0.9, 0.1, 0.8]),\n",
        "}\n",
        "#populate the dictionaty with the right shapes before using them as target set\n",
        "# i have to create a target set with 8 rows and 5 columns \n",
        "for i in target_dict:  \n",
        "  target_dict[i]= np.tile(target_dict[i], (30000,1))\n",
        "\n",
        "print([i.shape for i in target_dict.values()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kn_GW57YbNB"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSZSsOhRCq9K",
        "outputId": "1ed728e1-3c2d-4722-d76f-6d6037930dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lenght and shapes of my training datasets = 7, [(30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8), (30000, 1, 8)]\n",
            "mean and variance of arpeggio  = 0.240, 0.027\n",
            "mean and variance of strumming  = 0.310, 0.037\n",
            "mean and variance of bending  = 0.116, 0.032\n",
            "mean and variance of doublePick  = 0.314, 0.050\n",
            "mean and variance of strongPick  = 0.255, 0.037\n",
            "mean and variance of tapping  = 0.289, 0.032\n",
            "mean and variance of pullOffHammerOn  = 0.162, 0.028\n"
          ]
        }
      ],
      "source": [
        "pd_dataframe= {\n",
        "    'arpeggio': pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sEMG_pandas_dataset/csv_pd_dataframe/df_arpeggio_rms'),\n",
        "    'strumming': pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sEMG_pandas_dataset/csv_pd_dataframe/df_strumming_rms'),\n",
        "    'bending': pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sEMG_pandas_dataset/csv_pd_dataframe/df_bending_rms'),\n",
        "    'doublePick':pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sEMG_pandas_dataset/csv_pd_dataframe/df_doublePick_rms'),\n",
        "    'strongPick':pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sEMG_pandas_dataset/csv_pd_dataframe/df_strongPick_rms'),\n",
        "    'tapping':pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sEMG_pandas_dataset/csv_pd_dataframe/df_tapping_rms'),\n",
        "    'pullOffHammerOn': pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sEMG_pandas_dataset/csv_pd_dataframe/df_pullOffHammerOn_rms')\n",
        "}\n",
        "scaler = MinMaxScaler()\n",
        "#shaping all the pd dataset to a (30000, 8)\n",
        "for i in pd_dataframe:\n",
        "  pd_dataframe[i]= pd_dataframe[i].iloc[5000:35000].values # excluding the first and the last 5 second of samples\n",
        "  pd_dataframe[i] = scaler.fit_transform(pd_dataframe[i])\n",
        "  pd_dataframe[i]= np.reshape(pd_dataframe[i],(pd_dataframe[i].shape[0], 1, pd_dataframe[i].shape[1]))\n",
        "print('The lenght and shapes of my training datasets = {}, {}'.format(len(pd_dataframe), [pd_dataframe[key].shape for key in pd_dataframe]))\n",
        "\n",
        "#Printing some statistics \n",
        "for i in pd_dataframe.keys():\n",
        "  print('mean and variance of', i ,' = {:.3f}, {:.3f}'.format(np.mean(pd_dataframe[i]), np.var(pd_dataframe[i])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training dataset packing\n",
        "Here I pack my input data to feed them to the Model during the training process. Furthemore we split the Training dataset in two part, one for the training process and one for the evalution, called Test set."
      ],
      "metadata": {
        "id": "jZF-98R1pllD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "train_data= [(pd_dataframe['arpeggio'], target_dict['arpeggio']),\n",
        "             (pd_dataframe['strumming'], target_dict['strumming']),\n",
        "             (pd_dataframe['doublePick'], target_dict['doublePick']),\n",
        "             (pd_dataframe['tapping'], target_dict['tapping']),\n",
        "             (pd_dataframe['strongPick'], target_dict['strongPick']),\n",
        "             (pd_dataframe['pullOffHammerOn'], target_dict['pullOffHammerOn'])]\n",
        "\"\"\"\n",
        "concatenated_training_datasets= np.concatenate(([pd_dataframe[key] for key in pd_dataframe.keys()]),axis=0)\n",
        "#print(x_train.shape)\n",
        "concatenated_target_datasets= np.concatenate(([target_dict[key] for key in target_dict.keys()]), axis= 0)\n",
        "#print(y_train.shape)\n",
        "#splitting the dataset into a training and testing dataset\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(concatenated_training_datasets, concatenated_target_datasets, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
      ],
      "metadata": {
        "id": "p1NGW1SMpjka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5440c9-b8b0-437e-c995-44e9c0cb5625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(168000, 1, 8) (42000, 1, 8) (168000, 5) (42000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does not have any senso to put as a imput layer a dense layer. it distracts the LSTM from the time-series analysis, the input layer before the lstm is usualy used when you have a higher values of input features, and you want to do some inference before classify the time series data. The input dense of CNN layer is used to reduce the number of features (parameters) before feed the LSTM. In this case we do not need to use a input dense layer because the number of features is low. We can use it after the LSTM if it encrease the performance. "
      ],
      "metadata": {
        "id": "TOu13Y0rsjjB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83RPKjCW3wck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5caf7850-af11-42a8-9721-77ea3ee3a51f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output shape:  (None, 1, 5)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_7 (LSTM)               (None, 1, 64)             18688     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 1, 64)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1, 5)              325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,013\n",
            "Trainable params: 19,013\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "n_features = 8 # each time step has 1 feauture\n",
        "time_steps = #how many rows you want to elaborate togheter as a single time-stemp, if you want to track patter over time the time_stemps shold be larger\n",
        "\n",
        "input_shape= (time_steps, n_features)# each sample of new data is rappresented by 8 time steps in our dataset and each time step has feauture shape of 1\n",
        "#n_features is the number of features or variables at each time step.\n",
        "\n",
        "# create the input layer with 8 neurons\n",
        "#input_layer = Input(shape=input_shape)\n",
        "\n",
        "# pass the dataframe to the input layer\n",
        "#input_data = input_layer(df.values)\n",
        "\n",
        "#model = Sequential()\n",
        "#model.add(Dense(num_input_channels, input_shape=(time_steps, 8), activaction= 'relu'))\n",
        "#model.add(LSTM(64))\n",
        "#model.add(Dense(5, activation='sigmoid'))  \n",
        "\"\"\"\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(8,1)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dense(5, activation='sigmoid')\n",
        "])      \n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "#By setting the input shape to (8, 1), you are specifying that each sample consists of 8 time steps, each time step consisting of a single feature\n",
        "# Add an input dense layer with 8 neurons, one for each column in the input dataset\n",
        "#I can train the model w\n",
        "#model.add(Dense(8, input_shape= (1,8)))\n",
        "\n",
        "# Add an LSTM layer with 64 units\n",
        "model.add(LSTM(64, input_shape= (30,8),return_sequences=True, kernel_regularizer=l1(0.1))) # when you want to stack multiple LSTM \n",
        "#you have to set the return_sequences=True to not have shape problems. \n",
        "\n",
        "model.add(Dropout(0.3))  # add a dropout layer with dropout rate of 0.2\n",
        "model.add(LSTM(64))\n",
        "# Add a dense output layer with 5 units and linear activation\n",
        "model.add(Dense(5, activation='linear')) #return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False. IT should be set true only in the output layer \n",
        "\n",
        "print('The output shape: ',model.output_shape)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLR7VpVF5N4x"
      },
      "source": [
        "### Compiling the Model \n",
        "To compile the network (before training it) we have to choose the Optimizer and the loss function: \n",
        "\n",
        "The optimizer is the algorithm used to update the weights of the model during training, and 'adam' is a popular optimizer that adapts the learning rate of each weight based on the first and second moments of the gradients.\n",
        "\n",
        "The loss function is used to measure how well the model is performing on the training data and guides the optimization process. 'mse' stands for mean squared error, which is a common loss function used for regression problems where the goal is to minimize the difference between the predicted and actual values.\n",
        "\n",
        "Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
        "\n",
        "Learning rate: The learning rate determines how much the model weights are updated during training. A larger learning rate can lead to faster convergence, but can also cause the model to overshoot the minimum."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the availables parameters of the network"
      ],
      "metadata": {
        "id": "i-S7BQANlG1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCajTFLg5JCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ca08ba-3ea7-4b1f-8545-781cd43afce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "\n",
        "#model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
        "myOptimizer = keras.optimizers.Adam(lr=0.01)\n",
        "model.compile(optimizer= myOptimizer, loss='mse', metrics=['mse'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyg3QYog5wGb"
      },
      "source": [
        "### Training the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8VeGmGrs5tdN",
        "outputId": "33b280ab-9fc2-4c10-960b-1adc0e6539c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "3938/3938 [==============================] - 36s 7ms/step - loss: 0.2831 - mse: 0.0548 - val_loss: 0.0757 - val_mse: 0.0528\n",
            "Epoch 2/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0761 - mse: 0.0529 - val_loss: 0.0758 - val_mse: 0.0526\n",
            "Epoch 3/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0760 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0528\n",
            "Epoch 4/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0760 - mse: 0.0528 - val_loss: 0.0760 - val_mse: 0.0527\n",
            "Epoch 5/75\n",
            "3938/3938 [==============================] - 24s 6ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0760 - val_mse: 0.0526\n",
            "Epoch 6/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0756 - val_mse: 0.0527\n",
            "Epoch 7/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0767 - val_mse: 0.0527\n",
            "Epoch 8/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0758 - val_mse: 0.0527\n",
            "Epoch 9/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0760 - mse: 0.0528 - val_loss: 0.0755 - val_mse: 0.0526\n",
            "Epoch 10/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0763 - val_mse: 0.0526\n",
            "Epoch 11/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0766 - val_mse: 0.0526\n",
            "Epoch 12/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0764 - val_mse: 0.0527\n",
            "Epoch 13/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0757 - val_mse: 0.0526\n",
            "Epoch 14/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0765 - val_mse: 0.0526\n",
            "Epoch 15/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0753 - val_mse: 0.0526\n",
            "Epoch 16/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0764 - val_mse: 0.0527\n",
            "Epoch 17/75\n",
            "3938/3938 [==============================] - 24s 6ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0759 - val_mse: 0.0527\n",
            "Epoch 18/75\n",
            "3938/3938 [==============================] - 30s 8ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0760 - val_mse: 0.0527\n",
            "Epoch 19/75\n",
            "3938/3938 [==============================] - 24s 6ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0761 - val_mse: 0.0526\n",
            "Epoch 20/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0526\n",
            "Epoch 21/75\n",
            "3938/3938 [==============================] - 24s 6ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0760 - val_mse: 0.0526\n",
            "Epoch 22/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0761 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0526\n",
            "Epoch 23/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0761 - val_mse: 0.0526\n",
            "Epoch 24/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0760 - val_mse: 0.0526\n",
            "Epoch 25/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0757 - val_mse: 0.0526\n",
            "Epoch 26/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0526\n",
            "Epoch 27/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0761 - val_mse: 0.0526\n",
            "Epoch 28/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0760 - val_mse: 0.0526\n",
            "Epoch 29/75\n",
            "3938/3938 [==============================] - 24s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0763 - val_mse: 0.0527\n",
            "Epoch 30/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0758 - val_mse: 0.0526\n",
            "Epoch 31/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0761 - val_mse: 0.0526\n",
            "Epoch 32/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0764 - val_mse: 0.0526\n",
            "Epoch 33/75\n",
            "3938/3938 [==============================] - 29s 7ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0759 - val_mse: 0.0527\n",
            "Epoch 34/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0527\n",
            "Epoch 35/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0756 - val_mse: 0.0526\n",
            "Epoch 36/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0764 - val_mse: 0.0526\n",
            "Epoch 37/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0758 - val_mse: 0.0526\n",
            "Epoch 38/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0764 - val_mse: 0.0527\n",
            "Epoch 39/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0526\n",
            "Epoch 40/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0756 - val_mse: 0.0526\n",
            "Epoch 41/75\n",
            "3938/3938 [==============================] - 24s 6ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0756 - val_mse: 0.0526\n",
            "Epoch 42/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0757 - val_mse: 0.0526\n",
            "Epoch 43/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0760 - val_mse: 0.0527\n",
            "Epoch 44/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0766 - val_mse: 0.0526\n",
            "Epoch 45/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0758 - val_mse: 0.0527\n",
            "Epoch 46/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0526\n",
            "Epoch 47/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0758 - val_mse: 0.0526\n",
            "Epoch 48/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0763 - val_mse: 0.0526\n",
            "Epoch 49/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0756 - val_mse: 0.0527\n",
            "Epoch 50/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0762 - mse: 0.0528 - val_loss: 0.0761 - val_mse: 0.0526\n",
            "Epoch 51/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0761 - val_mse: 0.0526\n",
            "Epoch 52/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0765 - val_mse: 0.0526\n",
            "Epoch 53/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0765 - val_mse: 0.0526\n",
            "Epoch 54/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0761 - val_mse: 0.0526\n",
            "Epoch 55/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0756 - val_mse: 0.0526\n",
            "Epoch 56/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0766 - val_mse: 0.0526\n",
            "Epoch 57/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0759 - val_mse: 0.0526\n",
            "Epoch 58/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0759 - val_mse: 0.0526\n",
            "Epoch 59/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0526\n",
            "Epoch 60/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0759 - val_mse: 0.0526\n",
            "Epoch 61/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0765 - val_mse: 0.0526\n",
            "Epoch 62/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0526\n",
            "Epoch 63/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0762 - val_mse: 0.0526\n",
            "Epoch 64/75\n",
            "3938/3938 [==============================] - 25s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0764 - val_mse: 0.0526\n",
            "Epoch 65/75\n",
            "3938/3938 [==============================] - 27s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0757 - val_mse: 0.0526\n",
            "Epoch 66/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0765 - val_mse: 0.0526\n",
            "Epoch 67/75\n",
            "3938/3938 [==============================] - 26s 7ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0760 - val_mse: 0.0526\n",
            "Epoch 68/75\n",
            "3938/3938 [==============================] - 24s 6ms/step - loss: 0.0763 - mse: 0.0528 - val_loss: 0.0765 - val_mse: 0.0526\n",
            "Epoch 69/75\n",
            "1928/3938 [=============>................] - ETA: 10s - loss: 0.0764 - mse: 0.0528"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ccc9a38eb4c3>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#bath_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Load data from pandas dataframe dictionary\n",
        "X_1 = pd_dataframe['arpeggio'] # Return a Numpy representation of the DataFrame. Only the values in the DataFrame will be returned, the axes labels will be removed.\n",
        "#print(type(X_1))\n",
        "X_1= tf.stack(X_1)\n",
        "y_1 = target_dict['arpeggio']\n",
        "\n",
        "X_2 = pd_dataframe['strumming'] # Return a Numpy representation of the DataFrame. Only the values in the DataFrame will be returned, the axes labels will be removed.\n",
        "print(type(X_2))\n",
        "X_2= tf.stack(X_2)\n",
        "y_2 = target_dict['strumming']\n",
        "print('output shape ', y_1.shape, ' input shape  ', X_1.shape )\n",
        "\"\"\"\n",
        "#print( [pd_dataframe[key] for key in pd_dataframe.keys()])\n",
        "\n",
        "# Fit the model with different datates each associated to a specific output target\n",
        "#create a list of datasets tuple to train the model \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(X_train,Y_train, validation_split=0.25, epochs=75)\n",
        "\n",
        "#bath_size \n",
        "#history_2 = model.fit(pd_dataframe['strumming'], target_dict['strumming'], validation_split=0.2,epochs=10, batch_size=32)\n",
        "#history_3 = model.fit(pd_dataframe['doublePick'], target_dict['doublePick'], validation_split=0.2,epochs=10, batch_size=32)\n",
        "#history_4 = model.fit(pd_dataframe['tapping'], target_dict['tapping'], validation_split=0.2,epochs=10, batch_size=32)\n",
        "#history_5 = model.fit(pd_dataframe['strongPick'], target_dict['strongPick'], validation_split=0.2,epochs=10, batch_size=32)\n",
        "#history_6 = model.fit(pd_dataframe['pullOffHammerOn'], target_dict['pullOffHammerOn'], validation_split=0.2,epochs=10, batch_size=32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The validation_split argument in Keras fit method specifies the fraction of the training data to be used as validation data. The model will not be trained on the validation data, but rather the validation data will be used to evaluate the model's training performance over the epochs on data it has not seen before.\n",
        "\n",
        "For example, if validation_split=0.2, then 20% of the training data will be used as validation data and the remaining 80% will be used as the training data. The validation data will be used to evaluate the model's performance after each epoch during training. The validation set is a portion of the training set to evaluate the training process, instead to evaluate the network we have to use a different dataset called testing dataset."
      ],
      "metadata": {
        "id": "_AQz4ws7trsB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "fGSqdevx7_cD",
        "outputId": "6d481f43-bbb9-4022-e3f5-13d0ffa7f2a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e1cf4d68cd60>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# list all data in training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ],
      "source": [
        "# list all data in training history\n",
        "print(history.history.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-ycVD7Q7j_t"
      },
      "source": [
        "### Plotting the Loss function and the MSE\n",
        "We can track the training accurancy to avoid overfitting by plotting the loss function and the MSE for each epoch. The MSE is a common metric to evaluate regession task. The lower the loss, the better the model performance. The purpose of the loss function is to evaluate how well the model is learning the patterns in the data and adjusting its weights and biases to make accurate predictions. So, when you train a model, you try to minimize the loss function by adjusting the model's parameters. In other evaluation metrics, such as accuracy or F1 score, the higher the value, the better the model performance.\n",
        "Ploting the training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "SP-_v2HHFdQ_",
        "outputId": "3d6eaf54-64f5-4b19-97a0-63562c2c2687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-75ebb92dacfa>:3: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n",
            "  plt.subplot(1,2,1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-75ebb92dacfa>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model MSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "plt.figure(figsize=(16,5))\n",
        "plt.title('arpeggio')\n",
        "plt.subplot(1,2,1)\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['mse'])\n",
        "plt.plot(history.history['val_mse'])\n",
        "plt.title('Model MSE')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpvwQNv__Eyb"
      },
      "source": [
        "## Model Evaluation \n",
        "Testing the model whit a testing set computing the loss function for an unseen dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW3baRLk_Bad"
      },
      "outputs": [],
      "source": [
        "#divide in training set e test \n",
        "#the testing dataset have to be useen data but whith a trained target set, you can use a dataset belonging to a target that you did not train. \n",
        "#in the training the network see the label, in the tasting set it see the label only at the end, but shold be data from a studied label otherwise it has no sense. \n",
        "#it has to be data belonging to one of the category which you use during the training but unseen during the training stage.\n",
        "#you have to split the training dataset to obtai a testing dataset \n",
        "\n",
        "## Model evaluation \n",
        "model.evaluate(X_test,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkD5fyq3Fe3p"
      },
      "source": [
        "Now we have to actualy try the regression, we could use the method model.predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkraYxm_Fs-i"
      },
      "outputs": [],
      "source": [
        "# Reshape the data to match the input shape of the model\n",
        "new_data= pd_dataframe['bending'][2]\n",
        "\n",
        "new_data= np.reshape(new_data,(1,new_data.shape[0], new_data.shape[1]))\n",
        "print(new_data.shape)\n",
        "#new_data = new_data.reshape(1, -1)\n",
        "\n",
        "# Make a prediction using the model\n",
        "prediction = model.predict(new_data)\n",
        "print(prediction, prediction.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3towlp2lCKN8"
      },
      "source": [
        "## Exporting the model \n",
        "Now we want to save the model to use it in RawPower\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2SsSEscDGlT"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save('sEMG_regression.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0tTUBK8DoNd"
      },
      "source": [
        "be sure to have alla the requirements to run the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui58rZowDrlk"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt\n",
        "!pip freeze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqQzdy1UY_1C"
      },
      "source": [
        "## Further implementations "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0nfRdvQGYVa"
      },
      "source": [
        "To keep track of the patterns of each single channel separately, you can use a multichannel LSTM model. This model has separate LSTM cells for each channel, which can learn to extract relevant features from the channel's data independently. You can then combine the outputs from each channel's LSTM cell to produce a final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcoxHRZBGlDh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "# Define input shape for each channel\n",
        "input_shape = (timesteps, features_per_channel)\n",
        "\n",
        "# Define input layers for each channel\n",
        "input_channel1 = Input(shape=input_shape, name='input_channel1')\n",
        "input_channel2 = Input(shape=input_shape, name='input_channel2')\n",
        "input_channel3 = Input(shape=input_shape, name='input_channel3')\n",
        "input_channel4 = Input(shape=input_shape, name='input_channel4')\n",
        "input_channel5 = Input(shape=input_shape, name='input_channel5')\n",
        "input_channel6 = Input(shape=input_shape, name='input_channel6')\n",
        "input_channel7 = Input(shape=input_shape, name='input_channel7')\n",
        "input_channel8 = Input(shape=input_shape, name='input_channel8')\n",
        "\n",
        "# Define LSTM layers for each channel\n",
        "lstm_channel1 = LSTM(32, return_sequences=True)(input_channel1)\n",
        "lstm_channel2 = LSTM(32, return_sequences=True)(input_channel2)\n",
        "lstm_channel3 = LSTM(32, return_sequences=True)(input_channel3)\n",
        "lstm_channel4 = LSTM(32, return_sequences=True)(input_channel4)\n",
        "lstm_channel5 = LSTM(32, return_sequences=True)(input_channel5)\n",
        "lstm_channel6 = LSTM(32, return_sequences=True)(input_channel6)\n",
        "lstm_channel7 = LSTM(32, return_sequences=True)(input_channel7)\n",
        "lstm_channel8 = LSTM(32, return_sequences=True)(input_channel8)\n",
        "\n",
        "# Concatenate outputs from all LSTM layers\n",
        "concatenated = concatenate([lstm_channel1, lstm_channel2, lstm_channel3, lstm_channel4, lstm_channel5, lstm_channel6, lstm_channel7, lstm_channel8])\n",
        "\n",
        "# Define output layer\n",
        "output = Dense(5, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Define model\n",
        "model = Model(inputs=[input_channel1, input_channel2, input_channel3, input_channel4, input_channel5, input_channel6, input_channel7, input_channel8], outputs=output)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiRUj5c35AtG"
      },
      "source": [
        "Once your model is compiled, you can train it using a supervised learning approach, where you associate specific input sEMG data streams with specific output parameter values. You can do this by creating a dataset that consists of pairs of input-output data points, where each input sEMG data stream is associated with a corresponding output parameter value.\n",
        "\n",
        "Assuming you have a dataset with N input-output data pairs, you can train your model using the fit method in Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxdfebiR5C6E"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3PPrPtN5RhP"
      },
      "source": [
        "In this example, X_train is a 3D tensor of shape (N, timesteps, input_dim) containing the input sEMG data streams, and y_train is a 2D tensor of shape (N, 5) containing the associated output parameter values. We're using a batch size of 32 and training for 50 epochs.\n",
        "\n",
        "## Input definition: Tensor array\n",
        "In machine learning, a tensor is a mathematical object represented as a multi-dimensional array of numerical values. Tensors can be used to represent a wide variety of data types, including scalars (0-dimensional tensors), vectors (1-dimensional tensors), matrices (2-dimensional tensors), and higher-dimensional arrays (3-dimensional or higher-order tensors).\n",
        "\n",
        "In deep learning, tensors are used to represent both the input data and the parameters of the neural network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrBxl-Yu5Vvo"
      },
      "source": [
        "Finally, once your model is trained, you can use it to predict the output parameter values for new sEMG data streams in real-time using the predict method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUSt1lB85SNG"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykLoVabv6toN"
      },
      "source": [
        "In this example, X_new is a 3D tensor of shape (1, timesteps, input_dim) containing a new sEMG data stream, and y_pred is a 2D tensor of shape (1, 5) containing the predicted output parameter values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9n62q1k7-NY"
      },
      "source": [
        "# Dataset preparation \n",
        "## From pandas dataframe to Tensor\n",
        "To define a tensor from a Pandas DataFrame, you can use the .values attribute of the DataFrame, which returns a NumPy array containing the data in the DataFrame. You can then convert the NumPy array to a tensor using TensorFlow. \n",
        "### How a dataset have to look like in Keras\n",
        "in Keras, a dataset typically consists of two separate arrays: the input data (features) and the target data (labels). The input data array should have shape (num_samples, input_dim) where num_samples is the number of samples and input_dim is the number of input features. The target data array should have shape (num_samples, num_classes) where num_samples is the number of samples and num_classes is the number of output classes.\n",
        "\n",
        "Alternatively, the dataset can be represented as a tuple or list of (x, y) pairs, where x is the input data array and y is the target data array. This is often the format returned by Keras' built-in dataset utilities and can be passed directly to the fit() method of a Keras model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN5qYUVi6tLk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a sample DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, 3],\n",
        "    'B': [4, 5, 6],\n",
        "    'C': [7, 8, 9]\n",
        "})\n",
        "\n",
        "# Convert DataFrame to a tensor using TensorFlow\n",
        "tensor = tf.convert_to_tensor(df.values, dtype=tf.float32)\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TztEWF0iyqdG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JltOZy4CpWDH"
      },
      "source": [
        "\n",
        "Now we want the model to have 24 inputs (RMS value) and 5 output. We need to pack one specific acquisition\n",
        "\n",
        "We create a buffer and we populate it with the stream of data taken by python associating it a specific outbuffer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31EvKekppSrg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "# Initialize the input and output buffers\n",
        "input_buffer = []\n",
        "output_buffer = []\n",
        "\n",
        "# Define the buffer size and time interval for accumulating data\n",
        "buffer_size = 100\n",
        "time_interval = 1.0\n",
        "\n",
        "# Loop over the input stream\n",
        "while True:\n",
        "    # Read an input vector from the stream\n",
        "    input_vector = read_input_vector()\n",
        "\n",
        "    # Append the input vector to the input buffer\n",
        "    input_buffer.append(input_vector)\n",
        "\n",
        "    # Read the corresponding output value from the stream\n",
        "    output_value = read_output_value()\n",
        "\n",
        "    # Append the output value to the output buffer\n",
        "    output_buffer.append(output_value)\n",
        "\n",
        "    # If the buffer is full, train the model on the accumulated data\n",
        "    if len(input_buffer) == buffer_size:\n",
        "        # Convert the input and output buffers to numpy arrays\n",
        "        X_train = np.array(input_buffer)\n",
        "        y_train = np.array(output_buffer)\n",
        "\n",
        "        # Train the model on the labeled training data\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1p3gQWyzn2gnxFadfNFKfGRwEjlluf25f",
      "authorship_tag": "ABX9TyPz+EnSH1y0sONgWgRdtISq",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}